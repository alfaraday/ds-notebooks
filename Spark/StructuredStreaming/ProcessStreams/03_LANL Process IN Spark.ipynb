{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start here, we need to kick off our network data server.\n",
    "\n",
    "Go over to a terminal window and attach to the `data_server` container with:\n",
    "`docker attach data_server`.\n",
    "\n",
    "You'll end up at a terminal prompt.\n",
    "\n",
    "To start the server, all you need to do is type: `python3.6 server.py`\n",
    "\n",
    "The server should report its address and port number; you can now return to this notebook and start working through the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist things first, our imports.\n",
    "\n",
    "We start with our `SparkContext` and `SparkSession`, then import the data types we need for our network flow data.\n",
    "\n",
    "Finally we import a `StreamingContext` which will create our streaming data frame, and we translate the CSV to JSON on the server side, so we need the `json` module to parse our JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a few utility functions with some minor error checking here. These confirm that we are taking in JSON, and then we add the JSON to our data frame. If we uncomment the `df.show()` you can see the data come in as it is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(js, col):\n",
    "    try:\n",
    "        data = json.loads(js)\n",
    "        return [data.get(i) for i in col]\n",
    "    except Exception as e:\n",
    "        print(f\"returning an empty json\")\n",
    "        return []\n",
    "\n",
    "def convert_json2df(rdd, col):\n",
    "    ss = SparkSession(rdd.context)\n",
    "    if rdd.isEmpty():\n",
    "        print(f\"Empty RDD.\")\n",
    "        return\n",
    "    df = ss.createDataFrame(rdd, schema=col)\n",
    "    #df.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our fields - note that in this streaming context we can't just rely on the parser to interpret the structure and header. We need to do that ourselves.\n",
    "\n",
    "`cols` is used to check that we have actual JSON.\n",
    "`colStruct` is used to define our structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Time', 'Duration', 'SrcDevice', 'DstDevice', 'Protocol', 'SrcPort', \n",
    "        'DstPort', 'SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes']\n",
    "\n",
    "colStruct = StructType([\n",
    "    StructField('time', IntegerType(), True),\n",
    "    StructField('duration', IntegerType(), True),\n",
    "    StructField('srcdevice', StringType(), True),\n",
    "    StructField('dstdevice', StringType(), True),\n",
    "    StructField('protocol', IntegerType(), True),\n",
    "    StructField('srcport', StringType(), True),\n",
    "    StructField('dstport', StringType(), True),\n",
    "    StructField('srcpackets', IntegerType(), True),\n",
    "    StructField('dstpackets', IntegerType(), True),\n",
    "    StructField('srcbytes', IntegerType(), True),\n",
    "    StructField('dstbytes', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we kick off our streaming. We open a socket text stream that connects to our server. At this point you should begin seeing the server send data. We process our JSON, and then convert it to a data frame.\n",
    "\n",
    "Finally we run our query on the data, and update it as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with two working thread and batch interval of 5 seconds\n",
    "sc = SparkContext(\"local[*]\", \"NetworkApp\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "lines = ssc.socketTextStream(\"172.18.0.3\", 9009).map(lambda x: check_json(x, cols)).foreachRDD(lambda x: convert_json2df(x, colStruct))\n",
    "\n",
    "if lines is not None:\n",
    "    webservers = lines.select('dstdevice').where(df['dstport'] == 80).groupby('dstdevice').count().sort(desc('count')).show(10)\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find web servers: They're destination devices that have port 80. Which ones are the most active?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
