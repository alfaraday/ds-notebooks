{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started With Structured Streaming\n",
    "---------------------------------------\n",
    "\n",
    "Now that we have worked through some examples with batch data, it's time to turn our attention to Spark's Structured Streaming.\n",
    "\n",
    "Before you start, it's a good idea to read through the following articles:\n",
    "* [Structured Streaming in Apache Spark](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)\n",
    "* [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "These both give you a sense of how one can use streaming - where we can see an advantage, as well as some of the challenges we'll face.\n",
    "\n",
    "Make no mistake, either - streaming can be hard to get right. In fact, we'll warn you up front that this walkthrough can be challenging to get right. But, it's worth it!\n",
    "\n",
    "Before we dive into the code, let's step back and take the 10,000-foot view of our process. Keep this notebook open while you run through the streaming walkthrough and exercise - it should help you keep perspective on all the new things you'll learn here.\n",
    "\n",
    "Part 1: The Process\n",
    "------------------\n",
    "\n",
    "Our end goal today is to stream and process data in real time with Spark. There are a ton of streaming data sources out there (think Twitter streaming, Citibike data, NYC MTA train arrivals, and so on). Those are all excellent data sources (and you may even want to try your hand at processing one of those datasets for your exercise, or even your final project).\n",
    "\n",
    "However, all of those streaming datasets present one major challenge - they are all realtime, which means, how can we **really** know that we've got our data right? Getting the data right - formatting, insuring clean data, and so on - is probably the main challenge when working with streaming data. In fact, we're not going to actually do much heavy lifting on the machine learning side of things with this exercise. Instead we'll leverage the fact that it's easy to query and get running counts from a streaming dataset to answer a question.\n",
    "\n",
    "To get around the problem of unknown realtime data, we're going to stream a known dataset through our instance of Spark. There are several ways we could choose, but we'll keep it simple for this first exercise. We're going to grab some know data, and preprocess it into a series of JSON files in a single directory. Each JSON will be a set of newline terminated \"streams.\"\n",
    "\n",
    "After we've seen streaming in action for this simple case, we can then start to build a more complicated environment where our Spark instance interacts with Kafka, Zookeeper, and at least one data source.\n",
    "\n",
    "Our Dataset: Network Data from Los Alamos National Lab\n",
    "---------------------------------------------------\n",
    "\n",
    "Los Alamos has posted a huge amount of network data online in csv files for you to use. For the purposes of this walkthrough, they're WAY too large for our needs. For our exercise we're going to use a very small subset of the data - only about 2.5 minutes worth. But - in that time, we see about 500,000 flows. If you're interested, the data is located [here](https://csr.lanl.gov/data/2017.html)\n",
    "\n",
    "Each line in the file represents a \"conversation\" between two computers. The fields captured are:\n",
    "* Time: the start time of the conversation (in a proprietary timestamp format)\n",
    "* Duration: the length of the conversation (in seconds)\n",
    "* SrcDevice: name of the device that initiated the conversation.\n",
    "* DstDevice: name of the device that was requested.\n",
    "* Protocol: network protocol used (TCP, UDP, etc)\n",
    "* SrcPort: network port (0-65,536) on the originating device\n",
    "* DstPort: network port (0-65,536) on the destination device\n",
    "* SrcPackets: network packet count sent from the source to the destination.\n",
    "* DstPackets: network packet count sent from the destination to the source.\n",
    "* SrcBytes: byte count sent from the source to the destination.\n",
    "* DstBytes: byte count sent from the destination to the source.\n",
    "\n",
    "As you can imagine, there's a lot here. However, we're going to try to answer a relatively simple question:\n",
    "**From this data, can we identify which devices are the web servers?**\n",
    "\n",
    "We'll use some knowledge about network behavior to tackle this - and we'll simplify things even more. All we're going to do is rely on the fact that web servers typically communicate on one of two ports: **80 or 443**. So, if a computer requests one of those ports as the `dstPort` in a flow, it's likely that the destination device (`dstDevice`) is a web server. If we see that computer name come up repeatedly in our request list, then there's a good chance that device is a web server.\n",
    "\n",
    "So for our streaming exercise, we need to build a count query that processes streams as they come in and updates the count of web servers, then reports back to us what it sees.\n",
    "\n",
    "How are we going to do this?\n",
    "\n",
    "For our work today, we'll try to be very methodical. This is how we'll get there:\n",
    "1. We'll go out to the LANL site and grab some netflow data. The data comes bzipped, by day. You can choose whatever day you'd like; our example uses day 3.\n",
    "2. We'll do some initial processing on the data using a combination of Python code and the command line. We want to use the best tools for the job, and in this case we can use Pandas to unzip the dataset, add a header row, and then grab the rows we want. We'll use the command line to split the single 500,000 row CSV into 50 10,000 row CSV's. Finally, we'll run a short shell script to convert those CSVs into the JSONs we'll use in our streaming experiment.\n",
    "2. Before we stream anything in Spark, we'll take advantage of the fact that we have a static dataset. We'll go back to our knowledge of batch processing, and work out the query on the JSONs statically to answer our web server question.\n",
    "3. Now, finally, we'll perform the same query, but with streaming data. We'll process the data in real-time, and update our count of web servers as we go. By the time the data finishes processing, we should have identical results with our batch processing.\n",
    "\n",
    "\n",
    "Of course, this is a significant generalization and simplification of what we can do with structured streaming in Spark. Our goal is to demonstrate the principal, and allow you to see what you can do with it. In a production environment, there would likely be a processing layer between Spark and the incoming data. Typically this is managed in Kafka, which is great at ingesting multiple sources of data, and sending them to Spark in a known format. This helps with processing streaming data - which often is very fragile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
