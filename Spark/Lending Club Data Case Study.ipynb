{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our first taste of programming with Spark, we'll revisit the Lending Club dataset you first used earlier in the course. Instead of building a random forest though, we'll perform a logistic regression to determine the likelihood that someone will be approved for a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "* data loading: Pyspark can't naively load a csv from a URL so we want to copy the data file into our container and load from there. This isn't the case for something like HDFS however, you can load directly from Hadoop. For details see [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets)\n",
    "* Note on dropping columns - need to create a new dataframe. It won't remove columns in-place.\n",
    "\n",
    "* Data prep strategy:\n",
    "    1. Drop columns that have high portion of nulls. Only keep columns with fewer than 10 nulls total. Otherwise drop it.\n",
    "    2. From remaining columns, review the schema and type. Cast any string columns that should be numeric into the appropriate value.\n",
    "    3. Once all column types are correct, look at the categorical variables, and create the dummies as needed.\n",
    "    \n",
    "* Here is the big thing - the data then needs to be transformed from a multicolumn CSV dataframe into a label / feature RDD.\n",
    "    * The RDD has to have only two columns: labels and a vector with all the features.\n",
    "    * To achieve this we will combine remaining numeric variables and the dummy variables, then compress them into single row vectors.\n",
    "    \n",
    "* At that point data prep will be complete and we can run the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"/home/ds/notebooks/datasets/LoanStats3d.csv\"\n",
    "APP_NAME = \"Lending Club Random Forest Example\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 141107\n",
    "TRAINING_DATA_RATIO = 0.7\n",
    "RF_NUM_TREES = 10\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .master(SPARK_URL) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .options(header = \"true\", inferschema = \"true\") \\\n",
    "    .csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas().to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_now_delinq': 1,\n",
       " 'acc_open_past_24mths': 1,\n",
       " 'addr_state': 0,\n",
       " 'all_util': 399723,\n",
       " 'annual_inc': 0,\n",
       " 'annual_inc_joint': 420583,\n",
       " 'application_type': 0,\n",
       " 'avg_cur_bal': 0,\n",
       " 'bc_open_to_buy': 3963,\n",
       " 'bc_util': 4227,\n",
       " 'chargeoff_within_12_mths': 0,\n",
       " 'collection_recovery_fee': 0,\n",
       " 'collections_12_mths_ex_med': 0,\n",
       " 'debt_settlement_flag': 0,\n",
       " 'debt_settlement_flag_date': 415607,\n",
       " 'deferral_term': 419185,\n",
       " 'delinq_2yrs': 0,\n",
       " 'delinq_amnt': 0,\n",
       " 'desc': 421050,\n",
       " 'disbursement_method': 1,\n",
       " 'dti': 2,\n",
       " 'dti_joint': 420586,\n",
       " 'earliest_cr_line': 0,\n",
       " 'emp_length': 0,\n",
       " 'emp_title': 23872,\n",
       " 'funded_amnt': 0,\n",
       " 'funded_amnt_inv': 0,\n",
       " 'grade': 0,\n",
       " 'hardship_amount': 419185,\n",
       " 'hardship_dpd': 419185,\n",
       " 'hardship_end_date': 419185,\n",
       " 'hardship_flag': 1,\n",
       " 'hardship_last_payment_amount': 419185,\n",
       " 'hardship_length': 419185,\n",
       " 'hardship_loan_status': 419185,\n",
       " 'hardship_payoff_balance_amount': 419185,\n",
       " 'hardship_reason': 419185,\n",
       " 'hardship_start_date': 419185,\n",
       " 'hardship_status': 419185,\n",
       " 'hardship_type': 419184,\n",
       " 'home_ownership': 0,\n",
       " 'id': 421095,\n",
       " 'il_util': 402478,\n",
       " 'initial_list_status': 0,\n",
       " 'inq_fi': 399722,\n",
       " 'inq_last_12m': 399723,\n",
       " 'inq_last_6mths': 0,\n",
       " 'installment': 0,\n",
       " 'int_rate': 0,\n",
       " 'issue_d': 0,\n",
       " 'last_credit_pull_d': 10,\n",
       " 'last_pymnt_amnt': 0,\n",
       " 'last_pymnt_d': 294,\n",
       " 'loan_amnt': 0,\n",
       " 'loan_status': 0,\n",
       " 'max_bal_bc': 399723,\n",
       " 'member_id': 421095,\n",
       " 'mo_sin_old_il_acct': 12254,\n",
       " 'mo_sin_old_rev_tl_op': 0,\n",
       " 'mo_sin_rcnt_rev_tl_op': 0,\n",
       " 'mo_sin_rcnt_tl': 0,\n",
       " 'mort_acc': 0,\n",
       " 'mths_since_last_delinq': 203961,\n",
       " 'mths_since_last_major_derog': 298365,\n",
       " 'mths_since_last_record': 346681,\n",
       " 'mths_since_rcnt_il': 400285,\n",
       " 'mths_since_recent_bc': 3798,\n",
       " 'mths_since_recent_bc_dlq': 312494,\n",
       " 'mths_since_recent_inq': 44600,\n",
       " 'mths_since_recent_revol_delinq': 269357,\n",
       " 'next_pymnt_d': 218972,\n",
       " 'num_accts_ever_120_pd': 1,\n",
       " 'num_actv_bc_tl': 0,\n",
       " 'num_actv_rev_tl': 0,\n",
       " 'num_bc_sats': 0,\n",
       " 'num_bc_tl': 0,\n",
       " 'num_il_tl': 0,\n",
       " 'num_op_rev_tl': 0,\n",
       " 'num_rev_accts': 1,\n",
       " 'num_rev_tl_bal_gt_0': 0,\n",
       " 'num_sats': 0,\n",
       " 'num_tl_120dpd_2m': 19230,\n",
       " 'num_tl_30dpd': 0,\n",
       " 'num_tl_90g_dpd_24m': 0,\n",
       " 'num_tl_op_past_12m': 0,\n",
       " 'open_acc': 0,\n",
       " 'open_acc_6m': 399722,\n",
       " 'open_act_il': 399723,\n",
       " 'open_il_12m': 399723,\n",
       " 'open_il_24m': 399723,\n",
       " 'open_rv_12m': 399723,\n",
       " 'open_rv_24m': 399723,\n",
       " 'orig_projected_additional_accrued_interest': 419482,\n",
       " 'out_prncp': 0,\n",
       " 'out_prncp_inv': 0,\n",
       " 'payment_plan_start_date': 419185,\n",
       " 'pct_tl_nvr_dlq': 0,\n",
       " 'percent_bc_gt_75': 4239,\n",
       " 'policy_code': 1,\n",
       " 'pub_rec': 0,\n",
       " 'pub_rec_bankruptcies': 0,\n",
       " 'purpose': 1,\n",
       " 'pymnt_plan': 0,\n",
       " 'recoveries': 0,\n",
       " 'revol_bal': 0,\n",
       " 'revol_bal_joint': 421094,\n",
       " 'revol_util': 162,\n",
       " 'sec_app_chargeoff_within_12_mths': 421095,\n",
       " 'sec_app_collections_12_mths_ex_med': 421095,\n",
       " 'sec_app_earliest_cr_line': 421095,\n",
       " 'sec_app_inq_last_6mths': 421095,\n",
       " 'sec_app_mort_acc': 421095,\n",
       " 'sec_app_mths_since_last_major_derog': 421095,\n",
       " 'sec_app_num_rev_accts': 421095,\n",
       " 'sec_app_open_acc': 421095,\n",
       " 'sec_app_open_act_il': 421095,\n",
       " 'sec_app_revol_util': 421095,\n",
       " 'settlement_amount': 415608,\n",
       " 'settlement_date': 415608,\n",
       " 'settlement_percentage': 415608,\n",
       " 'settlement_status': 415608,\n",
       " 'settlement_term': 415608,\n",
       " 'sub_grade': 0,\n",
       " 'tax_liens': 0,\n",
       " 'term': 0,\n",
       " 'title': 132,\n",
       " 'tot_coll_amt': 0,\n",
       " 'tot_cur_bal': 0,\n",
       " 'tot_hi_cred_lim': 0,\n",
       " 'total_acc': 0,\n",
       " 'total_bal_ex_mort': 0,\n",
       " 'total_bal_il': 399723,\n",
       " 'total_bc_limit': 0,\n",
       " 'total_cu_tl': 399723,\n",
       " 'total_il_high_credit_limit': 0,\n",
       " 'total_pymnt': 0,\n",
       " 'total_pymnt_inv': 0,\n",
       " 'total_rec_int': 0,\n",
       " 'total_rec_late_fee': 0,\n",
       " 'total_rec_prncp': 0,\n",
       " 'total_rev_hi_lim': 1,\n",
       " 'url': 421094,\n",
       " 'verification_status': 0,\n",
       " 'verification_status_joint': 420584,\n",
       " 'zip_code': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns_to_drop = [key for key, value in null_counts[0].items() if value > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'member_id',\n",
       " 'emp_title',\n",
       " 'url',\n",
       " 'desc',\n",
       " 'title',\n",
       " 'mths_since_last_delinq',\n",
       " 'mths_since_last_record',\n",
       " 'revol_util',\n",
       " 'last_pymnt_d',\n",
       " 'next_pymnt_d',\n",
       " 'mths_since_last_major_derog',\n",
       " 'annual_inc_joint',\n",
       " 'dti_joint',\n",
       " 'verification_status_joint',\n",
       " 'open_acc_6m',\n",
       " 'open_act_il',\n",
       " 'open_il_12m',\n",
       " 'open_il_24m',\n",
       " 'mths_since_rcnt_il',\n",
       " 'total_bal_il',\n",
       " 'il_util',\n",
       " 'open_rv_12m',\n",
       " 'open_rv_24m',\n",
       " 'max_bal_bc',\n",
       " 'all_util',\n",
       " 'inq_fi',\n",
       " 'total_cu_tl',\n",
       " 'inq_last_12m',\n",
       " 'bc_open_to_buy',\n",
       " 'bc_util',\n",
       " 'mo_sin_old_il_acct',\n",
       " 'mths_since_recent_bc',\n",
       " 'mths_since_recent_bc_dlq',\n",
       " 'mths_since_recent_inq',\n",
       " 'mths_since_recent_revol_delinq',\n",
       " 'num_tl_120dpd_2m',\n",
       " 'percent_bc_gt_75',\n",
       " 'revol_bal_joint',\n",
       " 'sec_app_earliest_cr_line',\n",
       " 'sec_app_inq_last_6mths',\n",
       " 'sec_app_mort_acc',\n",
       " 'sec_app_open_acc',\n",
       " 'sec_app_revol_util',\n",
       " 'sec_app_open_act_il',\n",
       " 'sec_app_num_rev_accts',\n",
       " 'sec_app_chargeoff_within_12_mths',\n",
       " 'sec_app_collections_12_mths_ex_med',\n",
       " 'sec_app_mths_since_last_major_derog',\n",
       " 'hardship_type',\n",
       " 'hardship_reason',\n",
       " 'hardship_status',\n",
       " 'deferral_term',\n",
       " 'hardship_amount',\n",
       " 'hardship_start_date',\n",
       " 'hardship_end_date',\n",
       " 'payment_plan_start_date',\n",
       " 'hardship_length',\n",
       " 'hardship_dpd',\n",
       " 'hardship_loan_status',\n",
       " 'orig_projected_additional_accrued_interest',\n",
       " 'hardship_payoff_balance_amount',\n",
       " 'hardship_last_payment_amount',\n",
       " 'debt_settlement_flag_date',\n",
       " 'settlement_status',\n",
       " 'settlement_date',\n",
       " 'settlement_amount',\n",
       " 'settlement_percentage',\n",
       " 'settlement_term']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "145\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "print(len(df.columns))\n",
    "print(len(null_counts[0].keys()))\n",
    "print(len(null_columns_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = df.drop(*null_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "print(len(df_updated.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 421095\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of rows: %d\" % df_updated.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: integer (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: string (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: string (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: string (nullable = true)\n",
      " |-- delinq_2yrs: double (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- inq_last_6mths: string (nullable = true)\n",
      " |-- open_acc: integer (nullable = true)\n",
      " |-- pub_rec: integer (nullable = true)\n",
      " |-- revol_bal: integer (nullable = true)\n",
      " |-- total_acc: string (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: string (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_amnt: string (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- collections_12_mths_ex_med: string (nullable = true)\n",
      " |-- policy_code: integer (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- acc_now_delinq: integer (nullable = true)\n",
      " |-- tot_coll_amt: integer (nullable = true)\n",
      " |-- tot_cur_bal: integer (nullable = true)\n",
      " |-- total_rev_hi_lim: integer (nullable = true)\n",
      " |-- acc_open_past_24mths: integer (nullable = true)\n",
      " |-- avg_cur_bal: integer (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: integer (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: integer (nullable = true)\n",
      " |-- mort_acc: integer (nullable = true)\n",
      " |-- num_accts_ever_120_pd: integer (nullable = true)\n",
      " |-- num_actv_bc_tl: integer (nullable = true)\n",
      " |-- num_actv_rev_tl: integer (nullable = true)\n",
      " |-- num_bc_sats: integer (nullable = true)\n",
      " |-- num_bc_tl: integer (nullable = true)\n",
      " |-- num_il_tl: integer (nullable = true)\n",
      " |-- num_op_rev_tl: integer (nullable = true)\n",
      " |-- num_rev_accts: integer (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: integer (nullable = true)\n",
      " |-- num_sats: integer (nullable = true)\n",
      " |-- num_tl_30dpd: integer (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: integer (nullable = true)\n",
      " |-- num_tl_op_past_12m: integer (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: integer (nullable = true)\n",
      " |-- tax_liens: integer (nullable = true)\n",
      " |-- tot_hi_cred_lim: integer (nullable = true)\n",
      " |-- total_bal_ex_mort: integer (nullable = true)\n",
      " |-- total_bc_limit: integer (nullable = true)\n",
      " |-- total_il_high_credit_limit: integer (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- disbursement_method: string (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_updated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a ton of columns and we probably should look at reducing it further. Now let's look at only the columns that are strings, and check whether we should convert them to a numeric datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "categoricals = [col[0] for col in df_updated.dtypes if col[1] == 'string']\n",
    "print(len(categoricals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+---------+----------+--------------+\n",
      "|      term|int_rate|grade|sub_grade|emp_length|home_ownership|\n",
      "+----------+--------+-----+---------+----------+--------------+\n",
      "| 60 months|   9.80%|    B|       B3| 10+ years|      MORTGAGE|\n",
      "| 60 months|  12.88%|    C|       C2|    1 year|      MORTGAGE|\n",
      "| 36 months|  15.77%|    D|       D1|   2 years|      MORTGAGE|\n",
      "| 36 months|  10.78%|    B|       B4|   8 years|          RENT|\n",
      "| 36 months|   7.49%|    A|       A4| 10+ years|      MORTGAGE|\n",
      "+----------+--------+-----+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------------+--------+-----------+----------+------------------+\n",
      "|annual_inc|verification_status| issue_d|loan_status|pymnt_plan|           purpose|\n",
      "+----------+-------------------+--------+-----------+----------+------------------+\n",
      "|     65000|       Not Verified|Dec-2015| Fully Paid|         n|debt_consolidation|\n",
      "|     70000|       Not Verified|Dec-2015|    Current|         n|debt_consolidation|\n",
      "|    175000|       Not Verified|Dec-2015| Fully Paid|         n|  home_improvement|\n",
      "|    104000|    Source Verified|Dec-2015| Fully Paid|         n|debt_consolidation|\n",
      "|    109000|       Not Verified|Dec-2015| Fully Paid|         n|debt_consolidation|\n",
      "+----------+-------------------+--------+-----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+----------+-----+----------------+--------------+\n",
      "|zip_code|addr_state|  dti|earliest_cr_line|inq_last_6mths|\n",
      "+--------+----------+-----+----------------+--------------+\n",
      "|   660xx|        KS|23.84|        Nov-2003|             0|\n",
      "|   786xx|        TX| 26.4|        Feb-1988|             0|\n",
      "|   430xx|        OH| 18.5|        Aug-1997|             1|\n",
      "|   441xx|        OH|14.01|        Nov-2000|             2|\n",
      "|   226xx|        VA|26.02|        Dec-2001|             1|\n",
      "+--------+----------+-----+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+-------------------+---------+---------------+------------------+\n",
      "|total_acc|initial_list_status|out_prncp|last_pymnt_amnt|last_credit_pull_d|\n",
      "+---------+-------------------+---------+---------------+------------------+\n",
      "|       37|                  w|     0.00|        8529.37|          Nov-2017|\n",
      "|       29|                  w| 11038.45|         363.07|          Dec-2017|\n",
      "|       23|                  w|     0.00|       23456.38|          Sep-2017|\n",
      "|       31|                  w|     0.00|       33262.93|          Mar-2016|\n",
      "|       19|                  w|     0.00|       20807.39|          Apr-2017|\n",
      "+---------+-------------------+---------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------------+----------------+-------------+-------------------+--------------------+\n",
      "|collections_12_mths_ex_med|application_type|hardship_flag|disbursement_method|debt_settlement_flag|\n",
      "+--------------------------+----------------+-------------+-------------------+--------------------+\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "+--------------------------+----------------+-------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_updated.select(categoricals[:6]).show(5)\n",
    "df_updated.select(categoricals[6:12]).show(5)\n",
    "df_updated.select(categoricals[12:17]).show(5)\n",
    "df_updated.select(categoricals[17:22]).show(5)\n",
    "df_updated.select(categoricals[22:]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the following numeric:\n",
    "term, int_rate, annual_inc, inq_last_6mths, total_acc, out_prncp, dti, last_payment_amnt\n",
    "\n",
    "We'll drop the following columns:\n",
    "last_credit_pull_d, issue_d, zip_code, addr_state, earliest_cr_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df_updated = df_updated.withColumn('term', regexp_replace(df_updated['term'], \" months\", \"\").cast(\"int\"))\n",
    "df_updated = df_updated.withColumn('int_rate', regexp_replace(df_updated['int_rate'], \"%\", \"\").cast(\"float\"))\n",
    "df_updated = df_updated.withColumn('annual_inc', df_updated['annual_inc'].cast(\"int\"))\n",
    "df_updated = df_updated.withColumn('inq_last_6mths', df_updated['inq_last_6mths'].cast(\"int\"))\n",
    "df_updated = df_updated.withColumn('total_acc', df_updated['total_acc'].cast(\"int\"))\n",
    "df_updated = df_updated.withColumn('out_prncp', df_updated['out_prncp'].cast(\"float\"))\n",
    "df_updated = df_updated.withColumn('dti', df_updated['dti'].cast(\"float\"))\n",
    "df_updated = df_updated.withColumn('last_pymnt_amnt', df_updated['last_pymnt_amnt'].cast(\"float\"))\n",
    "\n",
    "df_final = df_updated.drop('last_credit_pull_d', 'issue_d', 'zip_code', 'addr_state', 'earliest_cr_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: integer (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: integer (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: integer (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- delinq_2yrs: double (nullable = true)\n",
      " |-- inq_last_6mths: integer (nullable = true)\n",
      " |-- open_acc: integer (nullable = true)\n",
      " |-- pub_rec: integer (nullable = true)\n",
      " |-- revol_bal: integer (nullable = true)\n",
      " |-- total_acc: integer (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: float (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_amnt: float (nullable = true)\n",
      " |-- collections_12_mths_ex_med: string (nullable = true)\n",
      " |-- policy_code: integer (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- acc_now_delinq: integer (nullable = true)\n",
      " |-- tot_coll_amt: integer (nullable = true)\n",
      " |-- tot_cur_bal: integer (nullable = true)\n",
      " |-- total_rev_hi_lim: integer (nullable = true)\n",
      " |-- acc_open_past_24mths: integer (nullable = true)\n",
      " |-- avg_cur_bal: integer (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: integer (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: integer (nullable = true)\n",
      " |-- mort_acc: integer (nullable = true)\n",
      " |-- num_accts_ever_120_pd: integer (nullable = true)\n",
      " |-- num_actv_bc_tl: integer (nullable = true)\n",
      " |-- num_actv_rev_tl: integer (nullable = true)\n",
      " |-- num_bc_sats: integer (nullable = true)\n",
      " |-- num_bc_tl: integer (nullable = true)\n",
      " |-- num_il_tl: integer (nullable = true)\n",
      " |-- num_op_rev_tl: integer (nullable = true)\n",
      " |-- num_rev_accts: integer (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: integer (nullable = true)\n",
      " |-- num_sats: integer (nullable = true)\n",
      " |-- num_tl_30dpd: integer (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: integer (nullable = true)\n",
      " |-- num_tl_op_past_12m: integer (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: integer (nullable = true)\n",
      " |-- tax_liens: integer (nullable = true)\n",
      " |-- tot_hi_cred_lim: integer (nullable = true)\n",
      " |-- total_bal_ex_mort: integer (nullable = true)\n",
      " |-- total_bc_limit: integer (nullable = true)\n",
      " |-- total_il_high_credit_limit: integer (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- disbursement_method: string (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [col[0] for col in df_final.dtypes if col[1] == 'string']\n",
    "categoricals.remove('loan_status')\n",
    "uniques = [df_final.select(col).distinct().count() for col in categoricals]\n",
    "categorical_unique_counts = dict(zip(categoricals, uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+--------------+-------------------+----------+------------------+-------------------+\n",
      "|grade|sub_grade|emp_length|home_ownership|verification_status|pymnt_plan|           purpose|initial_list_status|\n",
      "+-----+---------+----------+--------------+-------------------+----------+------------------+-------------------+\n",
      "|    B|       B3| 10+ years|      MORTGAGE|       Not Verified|         n|debt_consolidation|                  w|\n",
      "|    C|       C2|    1 year|      MORTGAGE|       Not Verified|         n|debt_consolidation|                  w|\n",
      "|    D|       D1|   2 years|      MORTGAGE|       Not Verified|         n|  home_improvement|                  w|\n",
      "|    B|       B4|   8 years|          RENT|    Source Verified|         n|debt_consolidation|                  w|\n",
      "|    A|       A4| 10+ years|      MORTGAGE|       Not Verified|         n|debt_consolidation|                  w|\n",
      "|    C|       C4| 10+ years|           OWN|    Source Verified|         n|debt_consolidation|                  w|\n",
      "|    D|       D5| 10+ years|      MORTGAGE|           Verified|         n|debt_consolidation|                  w|\n",
      "|    B|       B4|   3 years|          RENT|       Not Verified|         n|       credit_card|                  w|\n",
      "|    D|       D2|   5 years|          RENT|       Not Verified|         n|       credit_card|                  w|\n",
      "|    D|       D3| 10+ years|      MORTGAGE|           Verified|         n|             other|                  w|\n",
      "+-----+---------+----------+--------------+-------------------+----------+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------------+----------------+-------------+-------------------+--------------------+\n",
      "|collections_12_mths_ex_med|application_type|hardship_flag|disbursement_method|debt_settlement_flag|\n",
      "+--------------------------+----------------+-------------+-------------------+--------------------+\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   Y|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "|                         0|      Individual|            N|               Cash|                   N|\n",
      "+--------------------------+----------------+-------------+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.select(categoricals[:8]).show(10)\n",
    "df_final.select(categoricals[8:]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = []\n",
    "final_cols.append('loan_status')\n",
    "final_cols.extend(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.select(*final_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 55.0 failed 1 times, most recent failure: Lost task 1.0 in stage 55.0 (TID 2691, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-52-92eb4d49f41a>\", line 4, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 882, in dense\n    return DenseVector(elements)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 289, in __init__\n    ar = np.array(ar, dtype=np.float64)\nValueError: could not convert string to float: 'Fully Paid'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-52-92eb4d49f41a>\", line 4, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 882, in dense\n    return DenseVector(elements)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 289, in __init__\n    ar = np.array(ar, dtype=np.float64)\nValueError: could not convert string to float: 'Fully Paid'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-92eb4d49f41a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of training set rows: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of test set rows: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \"\"\"\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 55.0 failed 1 times, most recent failure: Lost task 1.0 in stage 55.0 (TID 2691, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-52-92eb4d49f41a>\", line 4, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 882, in dense\n    return DenseVector(elements)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 289, in __init__\n    ar = np.array(ar, dtype=np.float64)\nValueError: could not convert string to float: 'Fully Paid'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/spark/python/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-52-92eb4d49f41a>\", line 4, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 882, in dense\n    return DenseVector(elements)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 289, in __init__\n    ar = np.array(ar, dtype=np.float64)\nValueError: could not convert string to float: 'Fully Paid'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "transformed_df = df_final.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]\n",
    "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'application_type': 3,\n",
       " 'collections_12_mths_ex_med': 11,\n",
       " 'debt_settlement_flag': 3,\n",
       " 'disbursement_method': 2,\n",
       " 'emp_length': 13,\n",
       " 'grade': 7,\n",
       " 'hardship_flag': 3,\n",
       " 'home_ownership': 5,\n",
       " 'initial_list_status': 3,\n",
       " 'purpose': 15,\n",
       " 'pymnt_plan': 3,\n",
       " 'sub_grade': 35,\n",
       " 'verification_status': 4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_unique_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we need to convert the categoricals to dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dummies = [c+'_dmy' for c in categoricals]\n",
    " \n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_tmp')\n",
    "            for x in categoricals ]\n",
    " \n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y) \n",
    "            for x,y in zip(categoricals, categorical_dummies)]\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labeled sets\n",
    "final_cols = [col[0] for col in df_final.dtypes if col[1] != 'string'].extend(categorical_dummies)\n",
    "\n",
    "print(non_categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "labelIndexer = StringIndexer(inputCol='loan_status', outputCol=\"label\")\n",
    "tmp += [assembler_features, labelIndexer]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pipeline.fit(df_final).transform(df_final)\n",
    "allData.cache()\n",
    "trainingData, testData = allData.randomSplit([TRAINING_DATA_RATIO, 1-TRAINING_DATA_RATIO], seed=RANDOM_SEED) # need to ensure same split for each time\n",
    "print(\"Distribution of loan status in trainingData is: \", trainingData.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.select(\"loan_amnt\", \"int_rate\", \"loan_status\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"loan_status\", outputCol=\"indexed_loan_status\").fit(df_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import functools\n",
    "%matplotlib inline\n",
    " \n",
    "statuses = df_updated.groupBy('loan_status').count().collect()\n",
    "categories = [i[0] for i in statuses]\n",
    "counts = [i[1] for i in statuses]\n",
    " \n",
    "ind = np.array(range(len(categories)))\n",
    "width = 0.35\n",
    "plt.bar(ind, counts, width=width, color='r')\n",
    " \n",
    "plt.ylabel('counts')\n",
    "plt.title('Status distribution')\n",
    "plt.xticks(ind + width/2., categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
