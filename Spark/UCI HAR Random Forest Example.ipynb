{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest Classifier with Spark in Batch Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to get into Apache Spark, and the best way to do so is by experimenting with some code.\n",
    "\n",
    "We'll start by going over some terms, explain why we chose the dataset we'll use, and finally, build a classifier to identify movement type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How will we use Spark in this example?\n",
    "\n",
    "For this example, we'll use Apache Spark in a local, standalone mode. In fact, we're going to use Spark in this mode through most of the examples and exercises. This way, we can focus on the details of developing machine learning solutions with Spark.\n",
    "\n",
    "Once you're confident you can use local, standalone Spark in both batch and streaming mode, we'll work through how to create, deploy, and manage a Spark cluster in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Imports:\n",
    "\n",
    "Spark works a little different than the data science stack you've worked with already. In the data science stack, you are writing Python code that both executes immediately, and also executes locally on the device you're using.\n",
    "\n",
    "Spark, on the other hand, lives outside the Python environment. We treat it as a server that we pass instructions to; the Spark server performs the calculations, and then reports back to our notebook. Additionally, Spark uses the concept of a __pipeline__ in which we configure a number of steps, that are only executed when we tell the server to run.\n",
    "\n",
    "There are a number of benefits to this approach for big data. First of all, by using the server paradigm, we can run locally with a smaller set of data when we're building and testing code. Then, when we've successfully set up our model, deploying to production means we point our Python code to a remote server that is configured to handle larger datasets. Second, the pipeline approach allows us to configure models and run them through multiple steps more efficiently on big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these imports allow us to set up our Python connection to the Spark server.\n",
    "# it also allows us to load a dataframe.\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# these imports are how we build and manager our data science processes: cleaning data, preparing a model,\n",
    "# executing the model, and evaluating the model.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a set of constants for clarity and simplicity in managing the notebook.\n",
    "# this allows you to refer back to this cell at any time if you need to either confirm or modify any of these values.\n",
    "\n",
    "CSV_PATH = \"/home/ds/notebooks/datasets/UCI_HAR/allData.csv\"\n",
    "CSV_ACTIVITY_LABEL_PATH = \"/home/ds/notebooks/datasets/UCI_HAR/activity_labels.csv\"\n",
    "APP_NAME = \"UCI HAR Random Forest Example\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 141107\n",
    "TRAINING_DATA_RATIO = 0.8\n",
    "RF_NUM_TREES = 10\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started:\n",
    "\n",
    "1. Create the connection to the Spark server using the SparkSession.\n",
    "2. After that's done, import two datasets and do some cleaning and validation before we configure our model.\n",
    "\n",
    "We import two datasets:\n",
    "    1. `activity_labels`: a mapping of the classifier labels used in the datasets.\n",
    "    2. `df`: a generic name for the dataset we're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASIDE : THE UCI HAR Dataset\n",
    "\n",
    "We'll be using a slightly modified version of the HAR dataset. The source dataset is available [here](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones). If you download this, you'll see that there are actually four datasets:\n",
    "1. __Train__\n",
    "    * X_features\n",
    "    * Y_labels\n",
    "2. __Test__\n",
    "    * X_features\n",
    "    * Y_labels\n",
    "    \n",
    "To use this in Spark, we do a little bit of preparation outside this example. We'll describe the steps, in the event you choose to try to prep the data yourself (it's always a good exercise to flex your data munging muscles when you can).\n",
    "\n",
    "Here's what we did to generate the `allData.csv` file:\n",
    "1. The source files as provided are space-delimited. And, unfortunately, the files are inconsistent in the spacing. Most of the time there are only single space delimiters, but you find double spaces intermittently too. Those double spaces add extraneous columns to the dataset and create problems for our classifier.\n",
    "In order to get around this, we replaced the spaces with commas. It's not necessary but mainly for personal preferences. We also do a string replace to get rid of all double-delimiters and replace with single delimiters. \n",
    "When you've completed this step, you can import and should see that you have __561__ unique feature columns in both the train and test datasets.\n",
    "\n",
    "2. We also merge the features and labels datasets, then append the test dataset onto the train dataset.\n",
    "\n",
    "The final dataset should have __10,299__ rows and __562__ columns. Everything should be numeric - the labels will be integers, and the features doubles. Because the source data is already numeric, it's simpler to build and demonstrate our classifier in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
    "\n",
    "activity_labels = spark.read.options(inferschema = \"true\").csv(CSV_ACTIVITY_LABEL_PATH)\n",
    "\n",
    "df = spark.read.options(inferschema = \"true\").csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation\n",
    "\n",
    "As mentioned above, if we cleaned and prepared the data properly, the dataset will meet the following three conditions.\n",
    "\n",
    "Look over the datasets for:\n",
    "1. Final shape should be 10299 rows by 562 columns\n",
    "2. All feature columns should be doubles\n",
    "3. There should be no nulls\n",
    "    * This is important, since Spark will fail to build our vector variables that we need in our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we get going, we first create some diagnostic variables to conduct our tests.\n",
    "\n",
    "# Testing for data types\n",
    "# Use a list comprehension to grab the column names that all have the data type 'double'\n",
    "double_cols = [col[0] for col in df.dtypes if col[1] == 'double']\n",
    "\n",
    "# Testing for nulls in columns. \n",
    "# We use the dataframe select method to build a list that is then converted to a Python dict.\n",
    "# This way it's easy to sum up the nulls in a moment when we're actually testing for presence of nulls. \n",
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "                         for c in df.columns]).toPandas().to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, since we're using a notebook, we'll just pretty things up by using `print` statements to confirm the tests we mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is 10299 rows by 562 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 columns out of 562 total are type double.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(double_cols):d} columns out of {len(df.columns):d} total are type double.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 null values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {sum(null_counts[0].values()):d} null values in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up and running a classifier in Spark\n",
    "\n",
    "Assuming that the data is now clean, we are now ready to reshape the data and run the random forest model.\n",
    "\n",
    "In Spark we manipulate the data to work in a Spark pipeline, define each of the steps in the pipeline, chain them together in a pipeline, and finally run the pipeline.\n",
    "\n",
    "The Apache Spark classifiers expect two columns of input:\n",
    "1. __labels__: an indexed set of numeric variables that represent the classification from the set of features we provide.\n",
    "2. __features__: An indexed, vector variable that contains all of the feature values in each row. \n",
    "\n",
    "In order to do this we need to create these two columns.\n",
    "\n",
    "To create the indexed labels column, we create a column called `indexedLabel` using the `StringIndexer` method.\n",
    "To create the indexed features column, we need to do two steps:\n",
    "    1. Create the vector of features using the `VectorAssembler` method.\n",
    "    2. Create the indexed vector from the `features` column called `indexedFeatures`\n",
    "    \n",
    "The `indexedLabel` and `indexedFeatures` will be used in our random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our feature vector.\n",
    "# Note that we're doing the work on the `df` object - we don't create new dataframes, \n",
    "# just add columns to the one we already are using.\n",
    "\n",
    "# the transform method performs the act of creating the column.\n",
    "\n",
    "df = VectorAssembler(inputCols=double_cols, outputCol=\"features\").transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to confirm that the features are there.\n",
    "\n",
    "It's easy to do this in Apache Spark using the `select` and `show` methods on the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|            features|\n",
      "+---+--------------------+\n",
      "|  5|[0.289,-0.0203,-0...|\n",
      "|  5|[0.278,-0.0164,-0...|\n",
      "|  5|[0.28,-0.0195,-0....|\n",
      "|  5|[0.279,-0.0262,-0...|\n",
      "|  5|[0.277,-0.0166,-0...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"_c0\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to build the indexers, split our data for training and testing, define our model, and finally chain everything together into a pipeline.\n",
    "\n",
    "* __It is important to note - when we execute this cell, we're not actually running our model. We're only defining its parameters in this cell.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers / split data / classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"_c0\", outputCol=\"indexedLabel\").fit(df)\n",
    "\n",
    "# now generate the indexed feature vector.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell runs the pipeline - delivering a trained model at the end of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now easy to test our model and make predictions, simply by using the model's `transform` method on the `testData` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "\n",
    "We can use the MulticlassClassificationEvaluator to test the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps:\n",
    "\n",
    "So we've seen how to set up some data and build a classifier in Spark. You might want to play around with this notebook and learn more about how Spark works.\n",
    "\n",
    "Some ideas:\n",
    "1. Look at the set of labels, and see if there are features that might be better if combined. Spark has a means to map values into a new column.\n",
    "\n",
    "2. Identify the most important features among the 561 source features (using PCA or something similar); reduce the feature set and see if the model performs better.\n",
    "\n",
    "3. Modify the settings of the random forest to see if the performance improves.\n",
    "\n",
    "4. Use Spark's tools to find other techniques to evaluate the performance of your model - see if you can figure out how to generate a ROC plot, find the AUC value, or plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
